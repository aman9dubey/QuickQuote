{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# File imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "import codecs\n",
    "import re\n",
    "from datetime import datetime\n",
    "#from search_term import give_med_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "number = r'\\d{2,3}'\n",
    "gender = r'(\\b[Mm]ale?)|(\\b[Ff]emale?)|(\\bFEMALE)|(\\bMALE)|(/b)|F/|M/'\n",
    "Date = r'(([A-Z0-9][A-Z0-9]?[/-])?[A-Z0-9][A-Z0-9]?[/-][A-Z0-9][A-Z0-9][A-Z0-9]?[A-Z0-9]?)|([A-Za-z][A-Za-z][A-Za-z]\\s..?[,]\\s....)'\n",
    "DOB  = r'(.*)?DOB|[Dd][aA][tT][eE]\\s[oO][fF]\\s[Bb][iI][rR][tT][hH]\\s?(.*)?'\n",
    "year_four_digit = r'\\b(19|20)\\d{2}(w+)?'\n",
    "year_two_digit = r'\\d{2}$(w+)?'\n",
    "product_type = r'(\\b[Pp]roduct\\s[Tt]ype):\\s?.*'\n",
    "permanent = r'[Pp][eE][rR][mM]([aA][nN][aA][nN][tT])?'\n",
    "term = r'[tT][eE][rR][mM]'\n",
    "\n",
    "#Assuming USA currency dollar\n",
    "amount_with_dollar = r'(\\$\\s?\\d{1,3}(,\\d{2,3})*(\\.\\d+)?)(\\s?[kK]?)(\\s?[mM]?[mM]?(illion)?(ILLION)?)([bB]?)'\n",
    "amount_without_dollar = r'(\\$?\\s?\\d{1,3}(,\\d{2,3})*(\\.\\d+)?)(\\s?[kK]?)(\\s[mM]?[mM]?(illion)?(ILLION)?)([bB]?)((\\s?[Yy][Ee][aA][rR][sS]?)?)'\n",
    "faceamount = r'(\\b[Ff]ace\\s?[Aa]mount:?\\s?.*)'\n",
    "termamount = r'(.*)?[Tt][eE][rR][mM](.*)?'   \t\t\t#Regex to read single line from first newline to next newline\n",
    "seeking = r'(.*)?[Ss][eE][eE][kK]([iI][nN][gG])?(.*)?'\n",
    "term_year = r'(y(ea)?r|Y(ea)?r|Y(ea)?r)'\n",
    "\n",
    "weight = r'(.*)?\\b[wW][eE][iI][gG][hH][tT]\\s?(.*)?' \n",
    "weight_num = r'(\\d*\\.?\\d+)\\s?(lb|lbs|Lbs|LB|LBS|kg|Kg|KG|#)'\t\t#r'(.*)\\s?([lL][bB][sS]|[oO][zZ]|[gG]|[kK][Gg])' \n",
    "\n",
    "age_simple = r'(.*)?[Aa][Gg][Ee]\\s?(.*)?'\n",
    "age = r'(.*\\s?[Yy]([eE][aA])?[rR]?[sS]?\\s?([oO][lL][dD])?)'\n",
    "age_from_gender = r'(.*)?(\\b[Mm]ale?)|(\\b[Ff]emale?)|(\\bFEMALE)|(\\bMALE)|(/b)\\s?(.*)?' \n",
    "\n",
    "height_num = r'\\d{1,2}'\n",
    "height1 = r'((.*)?\\s?([Ff][eE][eE][tT])((.*)?\\s?([iI][nN][Cc][Hh][Ee][Ss]))?)'\t\t\t#Two types of inches => \"|”\n",
    "height2 = r'.[\\'|\\’](\\s?.[\\\"|\\”])?' \t\t\t\t\t\t\t\t\t\t\t\n",
    "feet = r'\\d[\\'|\\’]'\n",
    "inches = r'\\d[\\\"|\\”]' \n",
    "\n",
    "preferred = r'(.*)?(Preferred|preferred)\\s?(.*)?'\n",
    "height_word = r'Height|height'\n",
    "weight_word = r'Weight|weight' \n",
    "\n",
    "build = r'(Build|build)\\s?(.*)?'\n",
    "build_weight = r'\\d{3}'\n",
    "build_height = r'\\d\\.\\d'\n",
    "\n",
    "smoker = r'(.*)?[sS][Mm][oO][Kk]\\s?(.*)?' \n",
    "tobacco = r'(.*)?[Tt][oO][bB][aA][cC][cC][oO]\\s?(.*)?'\n",
    "no = r'[nN][oO]'\n",
    "\n",
    "med = r'(.*)?\\b[mM][eE][dD][iI][cC][aA][tT][iI][oO][nN]\\s?(.*)?'\n",
    "\n",
    "family = r'(.*)?(\\b[Ff]amily)\\s?(.*)?'\n",
    "family_member = r'(.*)?(\\b[Mm]om)|(\\b[Ff]ather)|(\\b[Dd]ad)|(\\b[Ss]ister)|(\\b[Bb]rother)|(\\b[Hh]usband)|(\\b[Ww]ife)\\s?(.*)?'\n",
    "\n",
    "lives = r'(.*)?(\\b[Ll]ives)\\s?(.*)?'\n",
    "prop = r'(.*)?(\\b[Pp]roperty)\\s?(.*)?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg(st,i):\n",
    "    for line in st: #iterate through every line\n",
    "        #return list of entities in that line\n",
    "        num = re.search(number, line, re.I | re.U)\n",
    "        x=0\n",
    "        x = re.search(Date, line, re.I | re.U)\n",
    "#Gender\n",
    "        y = re.search(gender, line, re.I | re.U)\n",
    "        if(y):\n",
    "            if(y.group(0)=='F/'):\n",
    "                data[i][0]='Female'\n",
    "            elif(y.group(0)=='M/'):\n",
    "                data[i][0]='Male'\n",
    "            else:\n",
    "\n",
    "                data[i][0]=(y.group(0))\n",
    "        elif(y and num):\n",
    "            data[i][0]=(y.group(0))\n",
    "        else:\n",
    "            data[i][0]=\" \"\n",
    "\n",
    "#Year for DOB\n",
    "        z = 0\n",
    "        x1 = re.search(year_four_digit, line, re.I | re.U)\n",
    "        if(x):\n",
    "            x1 = re.search(year_four_digit, x.group(0), re.I | re.U)\n",
    "            x2 = re.search(year_two_digit, x.group(0), re.I | re.U)\n",
    "            if(x1):\n",
    "                data[i][1]=x1.group(0)\n",
    "            elif(x2):\n",
    "                z = x2.group(0)\n",
    "                data[i][1] = '19'+z\n",
    "        elif(x1):\n",
    "            x1 = re.search(year_four_digit, line, re.I | re.U)\n",
    "\n",
    "            data[i][1]=x1.group(0)\n",
    "        else:\n",
    "            data[i][1]=\" \"\n",
    "\n",
    "#Age in years\n",
    "        age_reg = re.search(age, line, re.I | re.U)\n",
    "        age_simple_reg = re.search(age_simple, line, re.I | re.U)\n",
    "        dob = re.search(DOB, line, re.I | re.U)\n",
    "        age_gender_reg = re.search(age_from_gender, line, re.I | re.U)\n",
    "\n",
    "        if(age_gender_reg):\n",
    "                am = re.search(number, age_gender_reg.group(0), re.I | re.U)\n",
    "                if(am):\n",
    "                    data[i][2]=am.group(0)\n",
    "\n",
    "        if(x1):#20/03/1996\n",
    "\n",
    "            currentYear = datetime.now().year\n",
    "\n",
    "            data[i][2]=((currentYear-(int)(x1.group(0))))\n",
    "\n",
    "        else:\n",
    "            if(x1 and dob):#DOB 20/03/1996\n",
    "\n",
    "                currentYear = datetime.now().year\n",
    "\n",
    "                data[i][2]=((currentYear-(int)(x1.group(0))))\n",
    "            elif(x1 and y):#Male 20/03/1996\n",
    "\n",
    "                currentYear = datetime.now().year\n",
    "\n",
    "                data[i][2]=((currentYear-(int)(x1.group(0))))\n",
    "\n",
    "            else:\n",
    "                data[i][2]=' '\n",
    "\n",
    "            if(age_reg):#20 years ago\n",
    "                age_num = age_reg.group(0)\n",
    "                an = re.search(number, age_num, re.I | re.U)\n",
    "                if(an):\n",
    "\n",
    "                    data[i][2]=(an.group(0))\n",
    "\n",
    "            if(age_simple_reg):#Age 20\n",
    "                age_num = age_simple_reg.group(0)\n",
    "                an = re.search(number, age_num, re.I | re.U)\n",
    "                if(an):\n",
    "\n",
    "                    data[i][2]=(an.group(0))\n",
    "\n",
    "            if((data[i][2]<'18') and data[i][1]!=\" \"):#From Year of Birth\n",
    "                currentYear = datetime.now().year\n",
    "                data[i][2]=(currentYear-(int)(data[i][1]))\n",
    "\n",
    "#Product Type\n",
    "        z=re.search(product_type, line, re.I | re.U)\n",
    "        perm_reg = re.search(permanent, line, re.I | re.U)\n",
    "        term_type_reg = re.search(term, line, re.I | re.U)\n",
    "        if(z): \n",
    "\n",
    "            data[i][3]=(z.group(0))\n",
    "        elif(perm_reg):\n",
    "            final_str = \"Product Type: Permanent\"\n",
    "            data[i][3]=(final_str)\n",
    "        elif(term_type_reg):\n",
    "            final_str = \"Product Type: Term\"\n",
    "            data[i][3]=(final_str)\n",
    "        else:\n",
    "            data[i][3]=\" \"\n",
    "            \n",
    "#Face Amount\n",
    "\n",
    "        w = re.search(faceamount, line\t, re.I | re.U)\n",
    "        term_reg = re.search(termamount, line, re.I | re.U)\n",
    "        seek_reg = re.search(seeking, line, re.I | re.U)\n",
    "        \n",
    "#With faceAmount\n",
    "        if(w):\n",
    "            data[i][4]=(w.group(0))\n",
    "\n",
    "#With term Amount\n",
    "        elif(term_reg):\n",
    "            amd = re.search(amount_with_dollar, term_reg.group(0), re.I | re.U)\n",
    "            amwd = re.search(amount_without_dollar, term_reg.group(0), re.I | re.U)#Find 2nd regex in the same line of 1st regex \n",
    "\n",
    "            if(amd):\n",
    "                data[i][4]='Face Amount: '+(amd.group(0))\n",
    "            elif(amwd):\n",
    "                term_year_reg = re.search(term_year, amwd.group(0), re.I | re.U)\n",
    "                if(term_year_reg):\n",
    "                    data[i][4]='Term Year: '+(amwd.group(0))\n",
    "                else:\n",
    "                    data[i][4]='Face Amount: $'+(amwd.group(0))\n",
    "#With Seeking\n",
    "        elif(seek_reg):\n",
    "            amd = re.search(amount_with_dollar, seek_reg.group(0), re.I | re.U)\n",
    "            amwd = re.search(amount_without_dollar, seek_reg.group(0), re.I | re.U)\n",
    "        #Find 2nd regex in the same line of 1st regex \n",
    "            if(amd):\n",
    "                data[i][4]='Face Amount: '+(amd.group(0))\n",
    "            elif(amwd):\n",
    "                term_year_reg = re.search(term_year, amwd.group(0), re.I | re.U)\n",
    "                if(term_year_reg):\n",
    "                    data[i][4]='Term Year: '+(amwd.group(0))\n",
    "                else:\n",
    "                    data[i][4]='Face Amount: $'+(amwd.group(0))\n",
    "        else:\n",
    "            data[i][4]=\" \"\n",
    "\n",
    "#Weight\n",
    "        x=re.search(weight_num, line, re.I | re.U) \n",
    "        wt=re.search(weight, line, re.I | re.U)\n",
    "        if(x): \n",
    "#print (x.group(0)+\"\\n\")\n",
    "            data[i][5]=(x.group(0))\n",
    "        elif(wt):\n",
    "            am = re.search(weight_num,wt.group(0), re.I | re.U)\n",
    "            if(am):\n",
    "                data[i][5]=(am.group(0))\n",
    "        else:\n",
    "            data[i][5]=\" \"\n",
    "\n",
    "#Height\n",
    "        ht = re.search(height1, line, re.I | re.U)\n",
    "        htsym = re.search(height2, line, re.I | re.U)\n",
    "        if(ht): \n",
    "\n",
    "            data[i][6]=(ht.group(0))\n",
    "        elif(htsym):\n",
    "            f = re.search(feet, (htsym.group(0)), re.I | re.U)\n",
    "            inch = re.search(inches, (htsym.group(0)), re.I | re.U)\n",
    "            if(f):\n",
    "\n",
    "                am = re.search(height_num, (f.group(0)), re.I | re.U).group(0) + ' Feet'\n",
    "                if(i):\n",
    "                    am+=re.search(height_num, (inch.group(0)), re.I | re.U).group(0) + ' Inches' \n",
    "                data[i][6]=am\n",
    "        else:\n",
    "            data[i][6]=\" \"\n",
    "\n",
    "#Preferred Height & Weight\n",
    "        pr = ''\n",
    "        pr = re.search(preferred, line, re.I | re.U)\n",
    "        if(pr!='' and pr):\n",
    "            h_reg = re.search(height_word, pr.group(0), re.I | re.U)\n",
    "            if(h_reg):\n",
    "                data[i][6] = \"5 Feet 9 Inches\"\n",
    "            w_reg = re.search(weight_word, pr.group(0), re.I | re.U)\n",
    "            if(w_reg):\n",
    "                data[i][5] = \"196 lbs\"\n",
    "\n",
    "#Height & Weight from build\n",
    "        bu = ''\n",
    "        bu = re.search(build, line, re.I | re.U)\n",
    "        if(bu):\n",
    "            h_reg = re.search(build_height, bu.group(0), re.I | re.U)\n",
    "            if(h_reg):\n",
    "                data[i][6] = h_reg.group(0) + ' Feet'\n",
    "                h =' '\n",
    "            w_reg = re.search(build_weight, bu.group(0), re.I | re.U)\n",
    "            if(w_reg):\n",
    "                data[i][5] = w_reg.group(0)+ ' lbs'\n",
    "                w = ' '\n",
    "\n",
    "#Habit\n",
    "        sm = re.search(smoker, line, re.I | re.U)\n",
    "        tob = re.search(tobacco, line, re.I | re.U)\n",
    "        if(sm): \n",
    "            if(re.search(no, sm.group(0), re.I | re.U)):\n",
    "                data[i][7]=\"Non-Smoker\"\n",
    "            else:\n",
    "                data[i][7]=\"Smoker\"\n",
    "        elif(tob):\n",
    "\n",
    "            if(re.search(no, tob.group(0), re.I | re.U)):\n",
    "                data[i][7]=\"Non-Tobacco\"\n",
    "            else:\n",
    "                data[i][7]=\"Tobacco\"\n",
    "        else:\n",
    "            data[i][7]=\" \"\n",
    "\n",
    "\n",
    "#Medication & Treatment\n",
    "        med_reg = (re.search(med,line, re.I | re.U))\n",
    "        if(med_reg):\n",
    "            if(re.search(no, med_reg.group(0), re.I | re.U)):\n",
    "                data[i][8]=\"No Medication\"\n",
    "        else:#Write else outsite condition (to stop rewriting of above cell)\n",
    "            data[i][8]=\"\"\n",
    "\n",
    "#Family\n",
    "        family_reg = (re.search(family,line, re.I | re.U))\n",
    "        family_member_reg = (re.search(family_member,line, re.I | re.U))\n",
    "        if(family_reg):\n",
    "            data[i][9]=family_reg.groups()\n",
    "        else:#Write else outsite condition (to stop rewriting of above cell)\n",
    "            data[i][9]=\"\"\n",
    "\n",
    "#Property\n",
    "        lives_reg = (re.search(lives,line, re.I | re.U))\n",
    "        prop_reg = (re.search(prop,line, re.I | re.U))\n",
    "        if(lives_reg):\n",
    "            data[i][10]=lives_reg.groups()\n",
    "            if(prop_reg):\n",
    "                data[i][10]=lives_reg.groups()+prop_reg.groups()\n",
    "        elif(prop_reg):\n",
    "            data[i][10]=prop_reg.groups()\n",
    "        else:#Write else outsite condition (to stop rewriting of above cell)\n",
    "            data[i][10]=\"\"\n",
    "\n",
    "#medical data\n",
    "        #data[i][11] = give_med_terms(line)\n",
    "\n",
    "    wtr.writerows(data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "i=0\n",
    "w, h = 12, 1;\n",
    "data = [[\" \" for x in range(w)] for y in range(h)]\n",
    "st = []\n",
    "\n",
    "out = open('regexProcessed.csv', 'w', newline='')\n",
    "wtr= csv.writer( out )\n",
    "wtr.writerow(['Gender','Year_of_birth','Age(years)','Product Type','Face Amount','Weight','Height','Habit','Medication','Family','Property',''])\n",
    "\n",
    "\n",
    "\n",
    "with open('raw_data1.csv','r',encoding=\"ISO-8859-1\") as f:\n",
    "    rows = csv.reader(f)\n",
    "    for row in rows:\n",
    "\n",
    "        st.append(row[8])\n",
    "        reg(st,i)\n",
    "        st=[]\n",
    "          \n",
    "\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     FEMALE,1977,41,Product Type: Permanent,Face Am...\n",
       "2           male,1921,97, ,Face Amount: 250,000, , , , \n",
       "3     Male,1997,21, , ,200 lb,5 Feet, ,No Medication...\n",
       "4     Male,1945,73, ,Face Amount: $20,000 ,199#,8 Fe...\n",
       "5            Male, , ,Product Type: Permanent, , , , , \n",
       "6     Female,1966,52,Product Type: Term,Face Amount:...\n",
       "7     Female,1996,22,Product Type: Term,Face Amount:...\n",
       "8          MALE,1950,68, ,Face Amount: $300,000, , , , \n",
       "9                        Female,1950,68, , ,255lb, , , \n",
       "10    Female,1950,35,Product Type: Term,Face Amount:...\n",
       "11    Male, , ,Product Type: Term,Face Amount Ð 555K...\n",
       "12    Male, ,22,Product Type: Term,Term Year:  22 ye...\n",
       "13                    Female, ,33, , , , ,Non-Tobacco, \n",
       "14     ,1995,23,Product Type: Term,Face Amount: $ 59...\n",
       "15    Male,1995,55,Product Type: Term,Face Amount: $...\n",
       "16                                 ,1995,23, , , , , , \n",
       "17    Female,1996,22,Product Type: Term,Face Amount:...\n",
       "18    Female,1996,55,Product Type: Term,Term Year: 2...\n",
       "19    Male,1999,19, , ,199#, ,Non-Tobacco,('', 'Fami...\n",
       "Name: ColumnA, dtype: object"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"regexProcessed.csv\", encoding='ISO-8859-1')\n",
    "df = df.iloc[1:]\n",
    "df\n",
    "df['ColumnA'] = df[df.columns[0:]].apply(lambda x: ','.join(x.dropna().astype(str)),axis=1)\n",
    "df['ColumnA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the Pre-Processed File\n",
    "\n",
    "\n",
    "\n",
    "#process_data(name)\n",
    "\n",
    "#df = pd.read_csv(\"Processed_Data.csv\", encoding='ISO-8859-1')\n",
    "#df[\"Processed\"][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords,wordnet as wn\n",
    "from nltk.tokenize import wordpunct_tokenize,sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Removes all punctuations which acts as noise\n",
    "\n",
    "def rem_punt(doc):\n",
    "    ans = re.sub('\"|\\\\n|\\(|\\)|,|\\.|[$!--+@#:]',' ',doc)\n",
    "    ans = re.sub(' +',' ',ans)\n",
    "    ans = ans.lower()\n",
    "    return ans\n",
    "\n",
    "\n",
    "# Stop words removal using tokenization\n",
    "\n",
    "stop_word = set(stopwords.words('english'))\n",
    "\n",
    "def tokenize(document): \n",
    "    lemmy = []\n",
    "    for sent in sent_tokenize(document):\n",
    "        for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "            #print(token,tag)\n",
    "            if token in stop_word:\n",
    "                 continue\n",
    "            lemma = lemmatize(token, tag)\n",
    "            lemmy.append(lemma)\n",
    "    return lemmy\n",
    "\n",
    "#Lemmatization for tokens simplification\n",
    "\n",
    "def lemmatize(token, tag):\n",
    "    tag = {\n",
    "          'N': wn.NOUN,\n",
    "          'V': wn.VERB,\n",
    "          'R': wn.ADV,\n",
    "          'J': wn.ADJ\n",
    "    }.get(tag[0], wn.NOUN)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return lemmatizer.lemmatize(token, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[22]:\n",
    "\n",
    "df['Lemmitize'] = df['ColumnA'].apply(rem_punt).apply(tokenize)\n",
    "\n",
    "\n",
    "df.to_csv('NLPProcessed.csv',index=False, encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[25]:\n",
    "\n",
    "df = pd.read_csv('NLPProcessed.csv')\n",
    "\n",
    "\n",
    "# # Statistical Modeling \n",
    "\n",
    "# In[26]:\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer,LabelEncoder\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "X = df['Lemmitize']\n",
    "of = pd.read_csv('raw_data1.csv', encoding='ISO-8859-1')\n",
    "y = of['Offer']\n",
    "#y = df['Offer_noise_free']\n",
    "#lab_y = LabelEncoder()\n",
    "#y = lab_y.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['female', '1977', '41', 'product', 'type', 'permanent', 'face', 'amount', '50', '000', '200kg']\""
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[29]:\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y)\n",
    "\n",
    "\n",
    "# In[30]:\n",
    "\n",
    "vect = TfidfVectorizer(max_df=0.8, max_features=15000, min_df=0.01, use_idf=True , ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport matplotlib.pyplot as plt \\n%matplotlib inline\\nplt.spy(vect)\\n'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#from xgboost.sklearn import XGBClassifier\n",
    "#model1 = XGBClassifier(nthread=4,n_estimators=1000)\n",
    "\n",
    "\n",
    "# Naive Bayes\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB\n",
    "model2 = GaussianNB()\n",
    "\n",
    "\n",
    "# ExtraTree Classifier\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier,RandomForestClassifier\n",
    "model3 = RandomForestClassifier(n_estimators=600,n_jobs=-1)\n",
    "\n",
    "# SVM Classifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "model4 = SVC()\n",
    "\n",
    "\n",
    "# Logistic Regression \n",
    "\n",
    "from sklearn.linear_model import LinearRegression,SGDClassifier,LogisticRegression\n",
    "model5 = LogisticRegression()\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "model6 = LinearDiscriminantAnalysis()\n",
    "'''\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "plt.spy(vect)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model Fitting\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "\n",
    "name = [] \n",
    "results = []\n",
    "matrix_confusion = []\n",
    "training_time = []\n",
    "prediction_time = []\n",
    "def model_making(model_name, vect , model , X_train , y_train , X_test , y_test):\n",
    "    \n",
    "    t1 =time.time()\n",
    "    clf = make_pipeline(vect,model)\n",
    "    clf.fit(X_train,y_train)\n",
    "    t2 = time.time()\n",
    "    training_time.append(t2-t1)\n",
    "    \n",
    "    t1 = time.time()\n",
    "    pd = clf.predict(X_test)\n",
    "    t2 = time.time()\n",
    "    prediction_time.append(t2-t1)\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    name.append(model_name)\n",
    "    results.append(accuracy_score(y_test, y_pred)*100)\n",
    "    matrix_confusion.append(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    #print (\"=====Accuracy Score \", \"{0:.2f}\".format(accuracy_score(y_test, y_pred)*100), \"%\")\n",
    "    #print (\"=====Confusion Matrix\")\n",
    "    #print (confusion_matrix(y_test, y_pred))\n",
    "    #target_names = ['class 0', 'class 1', 'class 2']\n",
    "    #print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_making(\"Random Forest\",vect, model3, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_making(\"SVM\" , vect, model4, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_making(\"Logistic Regression\",vect, model5, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model_making(\"Naive Bayes\", vect , model2 , X_train, y_train, X_test, y_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset 19\n",
      "Training dataset:  14\n",
      "Testing dataset:  5 \n",
      "\n",
      "Name                       Accuracy         Training Time(s)    Prediction Time(s) \n",
      " \n",
      "Random Forest               40.000               1.778                        0.308s \n",
      " \n",
      "SVM                         40.000               0.137                        0.002s \n",
      " \n",
      "Logistic Regression         20.000               0.268                        0.002s \n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(\"Total dataset\",len(X))\n",
    "print(\"Training dataset: \",len(X_train))\n",
    "print(\"Testing dataset: \",len(X_test),\"\\n\")\n",
    "print(\"{:20} {:^20} {:^20} {:^20}\\n \".format(\"Name\" , \"Accuracy\" , \"Training Time(s)\" , \"Prediction Time(s)\" ) )\n",
    "\n",
    "for i in range(len(name)):\n",
    "    print(\"{:20} {:^20.3f} {:^20.3f} {:20.3f}s \\n \".format(name[i] , results[i] , training_time[i] , prediction_time[i] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    ['gender', 'male', 'dob', '10', '/', '0x', '/'...\n",
       "2    ['male', 'xx', '/', '31', '/', 'xx', '5', '’',...\n",
       "Name: Lemmitize, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
